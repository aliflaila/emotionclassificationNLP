{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T09:37:55.428708Z","iopub.execute_input":"2021-12-17T09:37:55.42923Z","iopub.status.idle":"2021-12-17T09:37:55.464895Z","shell.execute_reply.started":"2021-12-17T09:37:55.429114Z","shell.execute_reply":"2021-12-17T09:37:55.464071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/emotion-classification-nlp/emotion-labels-train.csv')\ntest = pd.read_csv('/kaggle/input/emotion-classification-nlp/emotion-labels-test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:55.466629Z","iopub.execute_input":"2021-12-17T09:37:55.467084Z","iopub.status.idle":"2021-12-17T09:37:55.515684Z","shell.execute_reply.started":"2021-12-17T09:37:55.467043Z","shell.execute_reply":"2021-12-17T09:37:55.514907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.columns,test.columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:55.516681Z","iopub.execute_input":"2021-12-17T09:37:55.516876Z","iopub.status.idle":"2021-12-17T09:37:55.522629Z","shell.execute_reply.started":"2021-12-17T09:37:55.516851Z","shell.execute_reply":"2021-12-17T09:37:55.521827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nsns.countplot(x = train.label)\nplt.xlabel(\"Label distribution of train data\")\nplt.subplot(1,2,2)\nsns.countplot(x = test.label)\nplt.xlabel(\"Label distribution of test data\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:55.524867Z","iopub.execute_input":"2021-12-17T09:37:55.525431Z","iopub.status.idle":"2021-12-17T09:37:56.870888Z","shell.execute_reply.started":"2021-12-17T09:37:55.525389Z","shell.execute_reply":"2021-12-17T09:37:56.869857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of labels in both train and test datasets is similar ","metadata":{}},{"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ndef clean_text(data):\n    stop = stopwords.words('english')\n    res = []\n    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    for x in data['text']:\n        x = str(x)\n        for punct in puncts:\n            if punct in x:\n                    x = x.replace(punct,' ')\n        res.append(x)\n    return res","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:56.872768Z","iopub.execute_input":"2021-12-17T09:37:56.873078Z","iopub.status.idle":"2021-12-17T09:37:56.887555Z","shell.execute_reply.started":"2021-12-17T09:37:56.873042Z","shell.execute_reply":"2021-12-17T09:37:56.886721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef word_count(df):\n    word_count = []\n    for i in df['text']:\n        word = i.split()\n        word_count.append(len(word))\n    return word_count\ntrain['word_count'] = word_count(train)\ntest['word_count'] = word_count(test)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:56.88889Z","iopub.execute_input":"2021-12-17T09:37:56.889088Z","iopub.status.idle":"2021-12-17T09:37:56.915461Z","shell.execute_reply.started":"2021-12-17T09:37:56.889064Z","shell.execute_reply":"2021-12-17T09:37:56.914636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_joy = train[train.label == 'joy']\ntrain_anger = train[train.label == 'anger']\ntrain_fear = train[train.label == 'fear']\ntrain_sadness = train[train.label == 'sadness']\ntest_joy = test[test.label == 'joy']\ntest_anger = test[test.label == 'anger']\ntest_fear = test[test.label == 'fear']\ntest_sadness = test[test.label == 'sadness']","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:56.917095Z","iopub.execute_input":"2021-12-17T09:37:56.917415Z","iopub.status.idle":"2021-12-17T09:37:56.935859Z","shell.execute_reply.started":"2021-12-17T09:37:56.917372Z","shell.execute_reply":"2021-12-17T09:37:56.935175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,4,1)\nplt.plot(train_joy.word_count)\nplt.xlabel(\"Word distribution of train data for class joy\")\nplt.subplot(1,4,2)\nplt.plot(train_anger.word_count)\nplt.xlabel(\"Word distribution of train data for class anger\")\nplt.subplot(1,4,3)\nplt.plot(train_fear.word_count)\nplt.xlabel(\"Word distribution of train data for class fear\")\nplt.subplot(1,4,4)\nplt.plot(train_sadness.word_count)\nplt.xlabel(\"Word distribution of train data for class sadness\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:56.937366Z","iopub.execute_input":"2021-12-17T09:37:56.937789Z","iopub.status.idle":"2021-12-17T09:37:57.60976Z","shell.execute_reply.started":"2021-12-17T09:37:56.937757Z","shell.execute_reply":"2021-12-17T09:37:57.59836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.subplot(1,4,1)\nplt.plot(test_joy.word_count)\nplt.xlabel(\"Word distribution of test data for class joy\")\nplt.subplot(1,4,2)\nplt.plot(test_anger.word_count)\nplt.xlabel(\"Word distribution of test data for class anger\")\nplt.subplot(1,4,3)\nplt.plot(test_fear.word_count)\nplt.xlabel(\"Word distribution of test data for class fear\")\nplt.subplot(1,4,4)\nplt.plot(test_sadness.word_count)\nplt.xlabel(\"Word distribution of test data for class sadness\")","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:57.612147Z","iopub.execute_input":"2021-12-17T09:37:57.61242Z","iopub.status.idle":"2021-12-17T09:37:58.273709Z","shell.execute_reply.started":"2021-12-17T09:37:57.612387Z","shell.execute_reply":"2021-12-17T09:37:58.273071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ny_train = le.fit_transform(train.label)\ny_test = le.transform(test.label)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:58.274827Z","iopub.execute_input":"2021-12-17T09:37:58.275435Z","iopub.status.idle":"2021-12-17T09:37:58.404953Z","shell.execute_reply.started":"2021-12-17T09:37:58.275401Z","shell.execute_reply":"2021-12-17T09:37:58.404263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train['word_count'].values.reshape(-1,1)\nX_test = test['word_count'].values.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:58.406162Z","iopub.execute_input":"2021-12-17T09:37:58.406729Z","iopub.status.idle":"2021-12-17T09:37:58.412054Z","shell.execute_reply.started":"2021-12-17T09:37:58.406683Z","shell.execute_reply":"2021-12-17T09:37:58.411182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state = 5,max_depth=4,splitter='best')\nclf.fit(X,y_train)\ny_preds = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:58.413066Z","iopub.execute_input":"2021-12-17T09:37:58.413286Z","iopub.status.idle":"2021-12-17T09:37:58.765578Z","shell.execute_reply.started":"2021-12-17T09:37:58.413259Z","shell.execute_reply":"2021-12-17T09:37:58.764888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=0.3)\nlr.fit(X,y_train)\ny_preds = lr.predict(X_test)\nacc_lr_score = accuracy_score(y_test,y_preds)\nprint(acc_lr_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:58.766863Z","iopub.execute_input":"2021-12-17T09:37:58.767152Z","iopub.status.idle":"2021-12-17T09:37:58.84598Z","shell.execute_reply.started":"2021-12-17T09:37:58.767123Z","shell.execute_reply":"2021-12-17T09:37:58.845144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using count vectorizer to vectorize the text variable**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nX_vec = vect.fit_transform(train['text'])\nX_test_vec = vect.transform(test['text'])\nclf.fit(X_vec,y_train)\ny_preds = clf.predict(X_test_vec)\n#from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:58.847646Z","iopub.execute_input":"2021-12-17T09:37:58.848153Z","iopub.status.idle":"2021-12-17T09:37:59.108501Z","shell.execute_reply.started":"2021-12-17T09:37:58.848109Z","shell.execute_reply":"2021-12-17T09:37:59.107596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_vec,y_train)\ny_preds = rf.predict(X_test_vec)\n#from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:37:59.110011Z","iopub.execute_input":"2021-12-17T09:37:59.110406Z","iopub.status.idle":"2021-12-17T09:38:02.96816Z","shell.execute_reply.started":"2021-12-17T09:37:59.110364Z","shell.execute_reply":"2021-12-17T09:38:02.967299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xgboost","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:38:02.969627Z","iopub.execute_input":"2021-12-17T09:38:02.970157Z","iopub.status.idle":"2021-12-17T09:38:12.766448Z","shell.execute_reply.started":"2021-12-17T09:38:02.970114Z","shell.execute_reply":"2021-12-17T09:38:12.765589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_vec,y_train)\ny_preds = model.predict(X_test_vec)\n#from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:38:12.768149Z","iopub.execute_input":"2021-12-17T09:38:12.768425Z","iopub.status.idle":"2021-12-17T09:38:14.9303Z","shell.execute_reply.started":"2021-12-17T09:38:12.768393Z","shell.execute_reply":"2021-12-17T09:38:14.929625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Try to use glove embeddings**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect_tf = TfidfVectorizer(ngram_range=(1,1),stop_words='english',max_features=3500)\nX_vec_tf = vect_tf.fit_transform(train['text'])\nX_test_vec_tf = vect_tf.transform(test['text'])\nmodel.fit(X_vec_tf,y_train)\ny_preds = model.predict(X_test_vec_tf)\n#from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:38:14.931555Z","iopub.execute_input":"2021-12-17T09:38:14.932002Z","iopub.status.idle":"2021-12-17T09:38:17.380609Z","shell.execute_reply.started":"2021-12-17T09:38:14.93196Z","shell.execute_reply":"2021-12-17T09:38:17.379881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf.fit(X_vec_tf,y_train)\ny_preds = rf.predict(X_test_vec_tf)\n#from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:38:17.384229Z","iopub.execute_input":"2021-12-17T09:38:17.386Z","iopub.status.idle":"2021-12-17T09:38:19.551586Z","shell.execute_reply.started":"2021-12-17T09:38:17.385962Z","shell.execute_reply":"2021-12-17T09:38:19.550615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\nimport lightgbm as lgb\n\ndef generate_embeddings(X_train,X_test,y_train,y_test):\n    def embed_document(data):\n        model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n        embeddings = np.array([np.array(model([i])) for i in data])\n        return pd.DataFrame(np.vstack(embeddings))\n    # vectorize the data\n    X_train_vec = embed_document(X_train)\n    X_test_vec = embed_document(X_test)\n    # USE doesn't have feature names\n    model = XGBClassifier(n_estimators=1000,learning_rate=0.001,max_depth=5,n_jobs=8)\n    #print(X_train_vec.shape,y_train.shape)\n    model.fit(X_train_vec, y_train)\n    model.score(X_test_vec, y_test)\n    ypred = model.predict(X_test_vec)\n    print('XGBoost scores')\n    #score = roc_auc_score(ypred,y_test)\n    #print(score)\n    accuracy = accuracy_score(y_test, ypred)\n    print(accuracy)\n    print(\"-------------------------------------------------------------------\")\n    print('LightGBM scores:')\n    clf = lgb.LGBMClassifier()\n    clf.fit(X_train_vec, y_train)\n    clf.score(X_test_vec, y_test)\n    ypred = clf.predict(X_test_vec)\n    #score = roc_auc_score(ypred,y_test)\n    #print(score)\n    accuracy = accuracy_score(y_test, ypred)\n    print(accuracy)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:38:19.554154Z","iopub.execute_input":"2021-12-17T09:38:19.554386Z","iopub.status.idle":"2021-12-17T09:38:26.782146Z","shell.execute_reply.started":"2021-12-17T09:38:19.55436Z","shell.execute_reply":"2021-12-17T09:38:26.781397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train['text']\nX_test = test['text']\ngenerate_embeddings(X_train,X_test,y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:38:26.783631Z","iopub.execute_input":"2021-12-17T09:38:26.783936Z","iopub.status.idle":"2021-12-17T09:50:03.947998Z","shell.execute_reply.started":"2021-12-17T09:38:26.783886Z","shell.execute_reply":"2021-12-17T09:50:03.947106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\ndef clean_text(data):\n    stop = stopwords.words('english')\n    res = []\n    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    for x in data['text']:\n        x = str(x)\n        for punct in puncts:\n            if punct in x:\n                    x = x.replace(punct,' ')\n        res.append(x)\n    return res","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:50:03.949865Z","iopub.execute_input":"2021-12-17T09:50:03.950826Z","iopub.status.idle":"2021-12-17T09:50:03.964364Z","shell.execute_reply.started":"2021-12-17T09:50:03.950785Z","shell.execute_reply":"2021-12-17T09:50:03.963775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nprocessed_train_text = clean_text(train)\nprocessed_test_text = clean_text(test)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nX_vec = vect.fit_transform(processed_train_text)\nX_test_vec = vect.transform(processed_test_text)\nmodel = XGBClassifier()\nmodel.fit(X_vec,y_train)\ny_preds = model.predict(X_test_vec)\n#from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test,y_preds)\nprint(acc_score)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:50:03.96949Z","iopub.execute_input":"2021-12-17T09:50:03.969713Z","iopub.status.idle":"2021-12-17T09:50:06.640058Z","shell.execute_reply.started":"2021-12-17T09:50:03.969685Z","shell.execute_reply":"2021-12-17T09:50:06.639261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gradio\nimport gradio as gr\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:50:06.650119Z","iopub.execute_input":"2021-12-17T09:50:06.650624Z","iopub.status.idle":"2021-12-17T09:50:25.3031Z","shell.execute_reply.started":"2021-12-17T09:50:06.650589Z","shell.execute_reply":"2021-12-17T09:50:25.302118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greet(input_tweet):\n    x = [input_tweet]\n    input_text = vect.transform(x)\n    X_test = pd.DataFrame.from_dict({'text':[input_text]}) \n    print(X_test)\n    y_predict = model.predict(X_test.values)\n    print(y_predict)\n    return y_predict[0]     \niface = gr.Interface( \n  fn = greet,\n  inputs=gr.inputs.Textbox(lines=5, placeholder=\"Enter your tweet here...\"),  \n  outputs=\"number\")\niface.launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:50:25.304326Z","iopub.execute_input":"2021-12-17T09:50:25.304581Z","iopub.status.idle":"2021-12-17T09:50:28.098064Z","shell.execute_reply.started":"2021-12-17T09:50:25.304531Z","shell.execute_reply":"2021-12-17T09:50:28.097043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = model.get_booster().feature_names\nx","metadata":{"execution":{"iopub.status.busy":"2021-12-17T09:50:28.099467Z","iopub.execute_input":"2021-12-17T09:50:28.099773Z","iopub.status.idle":"2021-12-17T09:50:28.104514Z","shell.execute_reply.started":"2021-12-17T09:50:28.099733Z","shell.execute_reply":"2021-12-17T09:50:28.103539Z"},"trusted":true},"execution_count":null,"outputs":[]}]}